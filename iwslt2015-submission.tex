\documentclass[a4paper]{article}
\usepackage{iwslt15,amssymb,amsmath,epsfig}
\usepackage{verbatim}
\setcounter{page}{1}
\sloppy		% better line breaks
%\ninept
%SM below a registered trademark definition
\def\reg{{\rm\ooalign{\hfil
     \raise.07ex\hbox{\scriptsize R}\hfil\crcr\mathhexbox20D}}}

%% \newcommand{\reg}{\textsuperscript{\textcircled{\textsc r}}}

\title{Integrating Semantic information into Neural Network Language Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please make sure to keep technical paper submissions anonymous  !
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\name{Firstname Lastname}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If multiple authors, uncomment and edit the lines shown below.       %%
%% Note that each line must be emphasized {\em } by itself.             %%
%% (by Stephen Martucci, author of spconf.sty).                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \makeatletter
% \def\name#1{\gdef\@name{#1\\}}
% \makeatother
% \name{{\em Firstname1 Lastname1, Firstname2 Lastname2, Firstname3 Lastname3,}\\
%      {\em Firstname4 Lastname4}}
%%%%%%%%%%%%%%% End of required multiple authors changes %%%%%%%%%%%%%%%%%

\address{Insitute for Anthropomatics  \\
Karlsruhe Institute of Technology, Germany \\
{\small \tt firstname.lastname@iwslt.org}
}
%
\begin{document}
\maketitle
%
\begin{abstract}
Neural models have recently shown big improvements in the performance of low-resource language modeling in phrase-based machine translation. Recurrent language models with different word factors, in particular, were a great success due to their ability to incorporate additional knowledge into the model. In this work, we want to integrate global semantic information extracted from large independent knowledge bases into neural network language models. We propose two approaches for doing this: word class extraction from Wikipedia and sentence level topic modeling. 
The new resulting models exhibit great potential in counteracting data scarcity problems with additional independent knowledge. This approach of integrating global context information is not restricted to language modeling but can also be easily applied to any model that profits from context or further data resources, e.g. neural machine translation. Using this model has improved rescoring quality of a state-of-the-art phrase-based translation system by ... BLEU points.  We performed experiments on two language pairs.



\end{abstract}


%
\section{Introduction}
Recurrent neural network language models have recently shown great improvement in statistical machine translation, both during decoding and rescoring. The use of continuous word representations has achieved better generalizations of the data which effectively lowered data sparseness problems. Furthermore, the recurrent connections are able to model long range dependencies. Yet, most of these models strictly depend on monolingual and parallel data, which is sometimes not available in huge amounts, especially for low-resource languages.
This has motivated neural network language models that take multiple parallel streams of data as input instead of just the single form of surface words. These so called factors can be used to add additional information, e.g. POS or automatic word clusters, which helps mainly with morphologically rich languages (e.g. Romanian, German). However, so far the use of factors or additional information has been limited in neural network models. Also, in those cases the extra feature only pertained to syntactic or local context knowledge around the current word. Especially for languages with low resources, it is essential to also facilitate the use of other knowledge factors, e.g. encyclopedia knowledge. It is a useful source especially for learning general concepts, even more after the emergence of the Internet has led to an explosion of textual data. These data sources give insights into a variety of human endeavors waiting to be computationally analyzed.
In this paper, we study the integration of large independent knowledge bases in the form of encyclopedia, e.g. Wikipedia, into RNN-based language models and propose two solutions. 
First, we use  the factored model to integrate extracted Wikipedia categories as one of the factors. In order to understand large unstructured datasets great achievements have been attained in latent concept learning in the area of text mining. Techniques include categorization of documents using latent semantic analysis and probabilistic topic modeling. In our case, we used techniques like tfidf, LSA and LDA to compute a real-valued topic vector for each sentence that is fed into the network as additional input. Using word classes and semantic features help both sparsely inflected languages(e.g. English, Chinese) \cite{bilmes2003factored} as well as low-resource languages.
In the model we use LSTMs to take into account both independent side information and local context information for the model prediction.


\section{Related Work}
Language models are a critical component of many application systems, e.g. ASR, MT and OCR. However, language models have always faced the problem of data sparseness. Factored Language Models \cite{bilmes2003factored} introduced the use of a bundle of factors associated with a word which outperformed previous n-gram models without expanding the training data. For factors morphs, stems, POS and word class obtained using the SRILM's n-gram-class tool were used. \cite{koehn2007factored} replaced the single feature stream of surface words with multiple factors and integrated it into phrase-based statistical machine translation systems by breaking down the translation model into several steps that pertain to the translation of single factors which are all taken into account when the target word is generated.
After recurrent neural network models became a success in language modeling \cite{mikolov2010recurrent}, a factored input layer was employed in a model by \cite{wu2012factored} which uses a structured output layer based on word classes that was able to handle vocabulary of arbitrary size.
Motivated by multi-task learning in NLP, \cite{niehuesusing} proposed a multi-factor recurrent neural network language model which jointly predicts different output factors by mapping the output of the LSTM-layer to as many softmax layers as there are output factors, thus creating multiple distributions at the output layer. In the rescoring of an n-best list, this model can be included as either one additional feature or several features depending on whether the output is treated as a joint probability or individual probabilities.
One disadvantage of factored input is that additional factors must match the surface words in space. As a consequence, surface words that uses 1-of-n encoding cannot have factors with continuous space representations.
However, this is often necessary to model more complex structures, e.g. topic distributions. 
In \cite{mikolov2012context}, a topic-conditioned RNNLM is proposed which takes a real-valued input vector as an additional input in association with each word. This vector is used to convey local context information based on previous sentences using latent dirichlet allocation.
Often, the meaning of a word cannot be just derived from its preceding words but by content words in the entire sentence or surrounding sentences. 
However, the model cannot take side information associated with a sentence that contains the current word, which is what we studied in the second part.

\section{Integration of Side Information}
We studied two approaches to integrate encyclopediatic information into neural language models.
First, we extracted for each word a corresponding label from Wikipedia. For this, we exploite the existing hierarchical page structure of Wikipedia and use the category name of a page as a factor input into the previously described factored neural language model.
Second, for each sentence a ranking of similar encyclopedia documents is calculated using vector space models, such as tfidf or LSA, and topic models, such as LDA. Based on the same underlying models a feature vector of the most similar documents to the current sentence is computed which is fed into a neural language model as additional input. The second approach is not limited to Wikipedia, but can be applied to any encyclopedia. To show the efficiency of this method independent from the underlying knowledge source we have crawled a Chinese encyclopedia for which we will present the results later on. 

\subsection{Word Level Information}
A Wikipedia page has an title and is either an article or a category page which encompasses one or multiple pages.
We define the search space as the set of all page titles. Given a word, we search for the page with the same word as title and retrieve its category which is found at the bottom of the page.
In case of multiple categories we pick the first one, which is usually the most general one.
Figure .. gives an example of this.
In implementation, we utilized the downloadable Wikipedia dump to create a offline mapping of pages and their category. Note that we skip categories that only have one element to reduce the overall number of associated categories. In case no category is found at all, we resort to the part-of-speech tag, a word's default label.
One typical characteristic of Wikipedia is that most of its articles only refer to a small group lexical categories. For example, there are lots more articles about objects (nouns) than about activities (verbs).
For this reason, we can significantly minimize the number of category lookups by restricting those to the words of the most common POS groups without taking big losses in model quality. We will show results for looking up every word versus just nouns. \\
\\



\subsection{Sentence Level Information}
To create a ranking of similar documents and their representation to a given sentence we employ vector space models, such as the tf-idf and latent semantic indexing model, as well as topic models, such as the latent dirichlet allocation. After representing the sentences and cleaned encyclopedia articles as bag-of-words with the underlying vocabulary from the training, we can find the most similar documents, which are the articles, for each query, which are the sentences, based on the underlying models.

\subsubsection{TF-IDF}
Tf-idf \cite{salton1986introduction} reflects how important a word is to a document or a whole collection of documents, its measures the co-occurrence. The advantage of tf-idf is that it grades down terms that appear in multiple documents which makes them less relevant.
The tf-idf is computed by multiplying a local component (term frequency or tf) with a global component (inverse document frequency or idf). Term frequency $tf(t, d)$ is defined as the number of times that term $t$ appears in document $d$:
\begin{equation} 
tf(t,d)=\dfrac{count_d(t)}{|d|}
\end{equation}
Inverse document frequency $idf(t, D)$ measures how much information a term $t$ provides regarding documents $D$, that is, whether it is common or rare in $D$:
\begin{equation}
idf(t,D)= \log_2{ \dfrac{|D|}{|\{d \in D: t \in d\}|}}
\end{equation}
\begin{equation}
tfidf(t, d, D) = tf(t,d) \cdot idf(t,D)
\end{equation}
Calculating tf-idf for each word in a vector gives an tf-idf vector. In gensim, this vector is normalized at the end.

To compute a ranking of similar documents for a given sentence query, we compute the query's tf-idf vector $q$ and compare it to each of the documents' tf-idf vectors $d_1, d_2, \ldots, d_{|D|}$ using the cosine to calculate the angle $\theta$ between vector pairs:
\begin{equation} \label{eq:cos-similar}
\cos \theta_i = \dfrac{q \cdot d_i}{|q| \cdot |d_i|}
\end{equation} 
The closer this value is to 1 the better the match.
For a predefined number of top documents $n$ we find the best n documents to a query and compute the average of their tfidf which we denote with $\text{top-doc-tfidf}(q, D, n)$.
This vector constitutes the sentence level semantic information which fed into a RNNLM together with the surface data stream.
Shortcomings of this model are that it does not reduce the description length of the document, since words are only replaced with values. Also, it does not tell much about the statistical structure within and between documents.


\subsubsection{Latent Semantic Indexing}
Another model we used is Latent Semantic Analysis (LSA) or Latent Semantic Indexing (LSI) \cite{deerwester1990indexing}.
LSI is a method for discovering hidden concepts in document data. Those hidden concepts are found using Single Value Decomposition(SVD) based on our documents. The derived LSI features are a linear combination of the original tf-idf features.
To calculate the similarity between a query q to any document d using equation \ref{eq:cos-similar}, we express both q and d in a unified way corresponding to these concepts, so that each of their vector elements gives the degree of participation of query or document in the corresponding concept. 

LSI uses a term-document matrix or occurrence matrix $A$, whose rows correspond to terms and columns correspond to documents. The entry $o_{i,j}$ is the tfidf-value of term i with respect to document j, that is $tfidf(t_i, d_j, D)$. SVD decomposing A into U, $\Sigma$, V so that $A = U \Sigma V^T $. U and V are orthonormal matrices and $\Sigma$ is a diagonal matrix whose diagonal elements are called single values.
LSI uses the decomposition to find a low-rank approximation, that is, a matrix $A_k$ of a predefined lower rank $k$ closes in similarity to the original matrix $A$. This is done by deleting all but the k biggest single values and the according rows and columns in V and S.
The degree of similarity is determined by the Frobenius Distance $\|A-A_k\|_F$, which is minimized by LSI. In our case, $k$ is the number of hidden concepts we want to learn. The number of dimensions k is an empirical question. However, crucially k is much smaller than the original space dimension, which is probably about the number of total documents. Previous papers show that for Wikipedia data containing between hundreds of thousands and a few million articles $k$ should be chosen between 200 to 500 \cite{bradford2008empirical}. We chose 300 for k.


\subsubsection{Latent Dirichlet Allocation}
Finally, we used Latent Dirichlet Allocation (LDA) \cite{blei2003latent}, a generative probabilistic model to automatically discover topics from a data collection.
The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. The model is a three-level hierarchical Bayesian model with the first level being the corpus-level, the second being the document-level and the third-level being the word-level. Having the number of topics to be learned set to K, following assumptions are made to generate a document \textbf{w} of length $N$ in a corpus $D$:
\begin{enumerate}
\item Choose the document length $N \sim Poisson(\xi)$
\item Choose document's distribution of topics $\theta \sim Dir(\alpha)$ with K dimensions
\item For each of the document word $w_n$:
\begin{enumerate}
\item Choose topic $z_n \sim Multinomial(\theta) $
\item Choose $ w_n$ from $p(w_n|z_n, \beta)$, a multinomial probability conditioned on the topic $z_n$. $\beta$ is a $K \times V$ matrix with $\beta_{ij}=p(w^j=1|z^i=1)$
\end{enumerate}
\end{enumerate}
The model is learning with Bayesian inference using collapsed Gibbs Sampling and expectation propagation. As for K, \cite{hoffman2010online} discusses how to choose the number of topics.
Given the computed model, we can predict the topic of a word or query using Bayes Theorem.
We chose 100 for K.

\section{Experiments}
\subsection{System Description}
\subsection{English-Chinese}
\subsection{English-Romanian}
\section{Conclusion}
\section{Acknowledgements}

\begin{comment}

The IWSLT 2015 organizing committee would like to thank the
organizing committees of INTERSPEECH 2004 for their
help and for kindly providing the template files.

\begin{itemize}
%\itemsep -1.3mm
\item Proceedings will be printed in A4 format. The layout is designed 
so that files, when printed in US Letter format, include all material 
but margins are not symmetric. 
Although this is not an absolute requirement, if at all possible,
{\bf PLEASE TRY TO MAKE YOUR SUBMISSION IN A4 FORMAT.}
\item Two columns are used except for the title part and possibly for large 
figures that need a full page width.
\item Left margin is 20 mm.
\item Column width is 80 mm.
\item Spacing between columns is 10 mm.
\item Top margin 25 mm (except first page 30 mm to title top).
\item Text height (without headers and footers) is maximum 235 mm.
\item Headers and footers must be left empty (they will be added for 
printing).
\item Check indentations and spacings by comparing to this 
example file (in pdf format).
\end{itemize}



\subsubsection{Headings}

Section headings are centered in boldface
with the first word capitalized and the rest of the heading in 
lower case. Sub-headings appear like major headings, except they 
start at the left margin in the column.
Sub-sub-headings appear like sub-headings, except they are in italics 
and not boldface. See the examples given in this 
file. No more than 3 levels of headings should be used.

\subsection{Text font}

Times or Times Roman font is used for the main text. Recommended 
font size is 9 points which is also the minimum allowed size.
Other font types may be used if needed for 
special purposes. While making the final PostScript file, 
remember to include all fonts!

\LaTeX\ users: DO NOT USE Computer Modern FONT FOR TEXT (Times is 
specified in the style file). If possible, make the final 
document using POSTSCRIPT FONTS.
This is necessary given that, for example, equations with 
non-ps Computer Modern are very hard to read on screen.

\subsection{Figures}

All figures must be centered on the column (or page, if the figure spans 
both columns).
Figure captions should follow each figure and have the format given in 
Fig.~\ref{spprod}.

Figures should preferably be line drawings. If they contain gray 
levels or colors, they should be checked to print well on a 
high-quality non-color laser printer.

\subsection{Tables}

An example of a table is shown as Table \ref{table1}. Somewhat 
different styles are allowed according to the type and purpose of the 
table. The caption text may be above or below the table.

\begin{table}
\caption{\label{table1} {\it This is an example of a table.}}
\vspace{2mm}
\centerline{
\begin{tabular}{|c|c|}
\hline
ratio & decibels \\
\hline  \hline
1/1 & 0 \\
2/1 & $\approx 6$ \\
3.16 & 10 \\
10/1 & 20 \\ 
1/10 & -20 \\
\hline
\end{tabular}}
\end{table}

\subsection{Equations}

Equations should be placed on separate lines and numbered. Examples 
of equations are given below.
Particularly,
%
%\vspace{-3mm}
\begin{equation}
x(t) = s(f_\omega(t))
\label{eq1}
\end{equation}
where \(f_\omega(t)\) is a special warping function
\begin{equation}
f_\omega(t)=\frac{1}{2\pi j}\oint_C \frac{\nu^{-1k}d\nu}
{(1-\beta\nu^{-1})(\nu^{-1}-\beta)}
\label{eq2}
\end{equation}
A residue theorem states that
\begin{equation}
\oint_C F(z)dz=2 \pi j \sum_k Res[F(z),p_k]
\label{eq3}
\end{equation}
Applying (\ref{eq3}) to (\ref{eq1}), 
it is straightforward to see that
\begin{equation}
1 + 1 = \pi
\label{eq4}
\end{equation}

Make sure to use \verb!\eqref! when refering to equation numbers.
Finally we have proven the secret theorem of all speech sciences (see
equation~\eqref{eq3} above).  No more math is needed to show how 
useful the result is! 

\begin{figure}[t]
\centerline{\epsfig{figure=figure,width=40mm}}
\caption{{\it Schematic diagram of speech production.}}  
\label{spprod}
\end{figure}

\subsection{Hyperlinks}

Hyperlinks can be included in your paper. Moreover, be aware that the paper
submission procedure includes the option of specifying a hyperlink for
additional information.  This hyperlink will be included in the CD-ROM.
Particularly pay attention to the possibility, from this single hyperlink, to
have further links to information such as other related documents, sound or
multimedia.

If you choose to use active hyperlinks in your paper, 
please make sure that they present no problems in printing to paper. 

\subsection{Page numbering}

Final page numbers will be added later to the document
electronically. 
{\em Please don't make any headers or footers!}.

\subsection{References}

The reference format is the standard for IEEE publications.
References should be numbered in order of appearance, 
for example \cite{ES1}, \cite{ES2}, and \cite{ES3}. 

\section{Experiments}
Please make sure to give all the necessary details regarding your experimental 
setting so as to ensure that your results could be reproduced by other teams. 

\section{Conclusions}

This paper has described a novel approach for doing wonderful stuff such as ...
content...
\end{comment}

%
\bibliographystyle{IEEEtran}
\bibliography{references}
%\begin{comment}
%\bibitem[1]{ES1} Smith, J. O. and Abel, J. S., 
%``Bark and {ERB} Bilinear Transforms'', 
%IEEE Trans. Speech and Audio Proc., 7(6):697--708, 1999.  
%\bibitem[2]{ES2} Lee, K.-F., Automatic Speech Recognition: 
%The Development of the 
%SPHINX SYSTEM, Kluwer Academic Publishers, Boston, 1989.
%\bibitem[3]{ES3} Rudnicky, A. I., Polifroni, Thayer, E. H.,
% and Brennan, R. A.  
%"Interactive problem solving with speech", J. Acoust. Soc. Amer., 
%Vol. 84, 1988, p S213(A).
%\end{comment}
\end{document}

